{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1aeeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 15:41:49.008253: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import dask.bag as db\n",
    "import dask.diagnostics as dd\n",
    "import random\n",
    "from termcolor import colored\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetV2M, EfficientNetV2B0, EfficientNetV2B3, Xception, ResNet50\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Precision, Recall, F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7705ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 19 15:41:51 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:4E:00.0 Off |                   On |\n",
      "| N/A   28C    P0             49W /  400W |                  N/A   |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                            |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                  |        ECC|                       |\n",
      "|==================+==================================+===========+=======================|\n",
      "|  0    2   0   0  |              38MiB / 19968MiB    | 42      0 |  3   0    2    0    0 |\n",
      "|                  |                 0MiB / 32767MiB  |           |                       |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05797d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "CWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b46be1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "SAVE_DIRECTORY = CWD + '/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64caf99",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baccb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(CWD, 'dataset', 'processed', 'Training')\n",
    "TEST_DIR = os.path.join(CWD, 'dataset', 'processed', 'Testing')\n",
    "CLASS_DIR = os.path.join(CWD, 'train_300', 'Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8b2e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glioma', 'meningioma', 'pituitary', 'notumor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop over folders to extract class_names\n",
    "classes = [class_name for class_name in os.listdir(CLASS_DIR) if os.path.isdir(os.path.join(CLASS_DIR, class_name)) and not class_name.startswith('.')]\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213af68b",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d95f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(BASE_DIR, IMG_SIZE, batch_size):\n",
    "    train_full = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=BASE_DIR,        \n",
    "        labels='inferred',         \n",
    "        label_mode='categorical',  \n",
    "        class_names=classes,       \n",
    "        seed=42,                   \n",
    "        batch_size=batch_size,             \n",
    "        image_size=(IMG_SIZE, IMG_SIZE)      \n",
    "    )\n",
    "\n",
    "    train_full = train_full.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Counting number of all batches in dataset\n",
    "    num_of_full_train_batches = len(list(train_full))\n",
    "    print(colored(f'Number of batches in train_full : {num_of_full_train_batches}', 'red', attrs=['bold']))\n",
    "    \n",
    "    # Define variable to store number of batches for train dataset\n",
    "    num_train_batches = int(num_of_full_train_batches * 0.70)\n",
    "    \n",
    "    # Define variable to store number of batches for validation and test dataset\n",
    "    num_valid_test_batches = num_of_full_train_batches - num_train_batches\n",
    "    \n",
    "    # Print the TARGET : number of batches for train, validation and test dataset to each\n",
    "    print(colored(' Target : ', 'green', attrs=['bold']))\n",
    "    print('-'*35)\n",
    "    print(colored(f'Number of  Train  batches : {num_train_batches}', 'blue', attrs=['bold']))\n",
    "    print(colored(f'Number of Validation batches : {num_valid_test_batches//2}', 'blue', attrs=['bold']))\n",
    "    print(colored(f'Number of Test batches : {num_valid_test_batches//2}', 'blue', attrs=['bold']))\n",
    "\n",
    "    # Apply above settings to main dataset to split to train, validation and test dataset\n",
    "    train_ds = train_full.take(num_train_batches)\n",
    "    remain_ds = train_full.skip(num_train_batches)\n",
    "    valid_ds = remain_ds.take(num_valid_test_batches)\n",
    "\n",
    "    train_ds = train_ds.shuffle(buffer_size=3)\n",
    "    \n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=TEST_DIR,\n",
    "        labels='inferred',\n",
    "        label_mode = 'categorical',\n",
    "        class_names=classes,\n",
    "        seed=42,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(IMG_SIZE, IMG_SIZE)\n",
    "    )\n",
    "\n",
    "    return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed6ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(BASE_DIR, TEST_DIR, IMG_SIZE, batch_size):\n",
    "    train_full = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=BASE_DIR,        \n",
    "        labels='inferred',         \n",
    "        label_mode='categorical',  \n",
    "        class_names=classes,       \n",
    "        seed=42,                   \n",
    "        batch_size=batch_size,             \n",
    "        image_size=(IMG_SIZE, IMG_SIZE)      \n",
    "    )\n",
    "\n",
    "    train_full = train_full.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Counting number of all batches in dataset\n",
    "    num_of_full_train_batches = len(list(train_full))\n",
    "    print(colored(f'Number of batches in train_full : {num_of_full_train_batches}', 'red', attrs=['bold']))\n",
    "    \n",
    "    # Define variable to store number of batches for train dataset\n",
    "    num_train_batches = int(num_of_full_train_batches * 0.70)\n",
    "    \n",
    "    # Define variable to store number of batches for validation and test dataset\n",
    "    num_valid_test_batches = num_of_full_train_batches - num_train_batches\n",
    "    \n",
    "    # Print the TARGET : number of batches for train, validation and test dataset to each\n",
    "    print(colored(' Target : ', 'green', attrs=['bold']))\n",
    "    print('-'*35)\n",
    "    print(colored(f'Number of  Train  batches : {num_train_batches}', 'blue', attrs=['bold']))\n",
    "    print(colored(f'Number of Validation batches : {num_valid_test_batches//2}', 'blue', attrs=['bold']))\n",
    "    print(colored(f'Number of Test batches : {num_valid_test_batches//2}', 'blue', attrs=['bold']))\n",
    "\n",
    "    # Apply above settings to main dataset to split to train, validation and test dataset\n",
    "    train_ds = train_full.take(num_train_batches)\n",
    "    valid_ds = train_full.take(num_valid_test_batches)\n",
    "\n",
    "    train_ds = train_ds.shuffle(buffer_size=3)\n",
    "    \n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=TEST_DIR,\n",
    "        labels='inferred',\n",
    "        label_mode = 'categorical',\n",
    "        class_names=classes,\n",
    "        seed=42,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(IMG_SIZE, IMG_SIZE)\n",
    "    )\n",
    "\n",
    "    return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f6410",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956befd",
   "metadata": {},
   "source": [
    "### Not Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb396a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_not_trainable(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "\n",
    "    # Freeze all layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa10485",
   "metadata": {},
   "source": [
    "### First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_trainable_first25(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze last 75% layers\n",
    "    total_layers = len(base_model.layers)\n",
    "    for layer in base_model.layers[(int(0.75 * total_layers)):]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a59c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_trainable_first50(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze last 50% layers\n",
    "    total_layers = len(base_model.layers)\n",
    "    for layer in base_model.layers[(int(0.50 * total_layers)):]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_trainable_first75(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze last 25% layers\n",
    "    total_layers = len(base_model.layers)\n",
    "    for layer in base_model.layers[(int(0.25 * total_layers)):]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc83361",
   "metadata": {},
   "source": [
    "### Last Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18dafbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_trainable_last25(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze first 75% layers\n",
    "    total_layers = len(base_model.layers)\n",
    "    for layer in base_model.layers[:(int(0.75 * total_layers))]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940406a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_trainable_last50(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze first 50% layers\n",
    "    total_layers = len(base_model.layers)\n",
    "    for layer in base_model.layers[:(int(0.50 * total_layers))]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee29a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_trainable_last75(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze first 25% layers\n",
    "    total_layers = len(base_model.layers)\n",
    "    for layer in base_model.layers[:(int(0.25 * total_layers))]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33a862",
   "metadata": {},
   "source": [
    "### Full Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3146c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_full_trainable(base_model, input_shape, num_classes):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = base_model(input_shape=input_shape, include_top=False, classes=num_classes, weights='imagenet')\n",
    "\n",
    "    # Full Trainable\n",
    "    base_model.trainable = True\n",
    "\n",
    "    x = base_model(input, training=False)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)               \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score()]\n",
    "    )\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15549ec1",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d59162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, train_ds, valid_ds, epochs, save_directory):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    saving_path = os.path.join(save_directory, f'model_{model_name}.h5')\n",
    "\n",
    "    # Model CheckPoint Call-Back, to save best model parameters as a .keras file\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(saving_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "    # Early Stoping Call-Backc to stop trainig process after 'patience' epochs if the metric doesn't grow\n",
    "    #earlystop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "    # ReduceLROnPlateau Call-Back to decrease learning-rate base on 'monitor' parameter after 'patience' epochs with a 'factor' is doesn't improve\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Model training\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data = valid_ds,\n",
    "        callbacks=[checkpoint_cb, reduce_lr]\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Counting trianing time\n",
    "    training_time = end - start\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "    return model, history, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222147c7",
   "metadata": {},
   "source": [
    "## Best Model Selection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_variant(models, test_ds, output_csv, output_folder='results'):\n",
    "    \"\"\"\n",
    "    Compare multiple models, select the best based on F1 Score, and save results to a CSV.\n",
    "\n",
    "    Args:\n",
    "        models (list of tuples): List of (model_name, model, history, time) tuples.\n",
    "        test_ds: The test dataset for evaluation.\n",
    "        output_csv (str): The name of the output CSV file.\n",
    "        output_folder (str): The directory to save the results.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing the best model's name, test loss, test accuracy, test F1 score, and training time.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Evaluate each model and collect results\n",
    "    models_info = []\n",
    "\n",
    "    for model_name, model, history, time in models:\n",
    "        # Assume val_accuracy as a proxy for F1 score\n",
    "        val_f1score = max(history.history['val_accuracy'])\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        val_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "        # Store each model's information in models_info\n",
    "        models_info.append({\n",
    "            'Model Name': model_name,\n",
    "            'Model': model,\n",
    "            'Loss': val_loss,\n",
    "            'Accuracy': val_acc,\n",
    "            'F1 Score': val_f1score,\n",
    "            'Training Time': time,\n",
    "            'History': history\n",
    "        })\n",
    "\n",
    "    # Sort the models by F1 Score in descending order\n",
    "    models_info = sorted(models_info, key=lambda x: x['F1 Score'], reverse=True)\n",
    "\n",
    "    # Convert to DataFrame for a neat tabular display\n",
    "    df = pd.DataFrame([{k: v for k, v in m.items() if k != 'Model' and k != 'History'} for m in models_info])\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(output_folder, output_csv)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Model comparison table saved to {output_path}\")\n",
    "\n",
    "    # Print DataFrame\n",
    "    print(\"Model Comparison Table:\")\n",
    "    print(df)\n",
    "\n",
    "    # Select the best model based on F1 Score\n",
    "    best_model_info = models_info[0]\n",
    "    best_model = best_model_info['Model']\n",
    "    best_model_name = best_model_info['Model Name']\n",
    "    best_history = best_model_info['History']\n",
    "    best_time = best_model_info['Training Time']\n",
    "\n",
    "    # Evaluate on the test dataset using plot function\n",
    "    best_history, test_loss, test_acc, test_f1score = plot(best_model, best_history, test_ds, f\"results/{best_model_name}\",  best_model_name)\n",
    "\n",
    "    # Return the best model's evaluation results for updating result_dict later\n",
    "    return best_model_name, test_loss, test_acc, test_f1score, best_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa14707",
   "metadata": {},
   "source": [
    "## Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, history, test_ds, output_folder='results', prefix='model'):\n",
    "    \"\"\"\n",
    "    Plot training results, confusion matrix, and save plots to files.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        history: Training history object from model.fit().\n",
    "        test_ds: Test dataset for evaluation.\n",
    "        output_folder (str): Directory to save plots.\n",
    "        prefix (str): Prefix for output file names.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing history, test loss, test accuracy, and test F1 score.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Convert result of training to DataFrame\n",
    "    result = pd.DataFrame(history.history)\n",
    "\n",
    "    # Define a variable to store range of epochs\n",
    "    x = np.arange(len(result))\n",
    "\n",
    "    # Create a plot with 2 rows and 1 column with size of (15, 12)\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "    # AX0 : Loss\n",
    "    ax[0].plot(x, result.loss, label='Loss', linewidth=3)\n",
    "    ax[0].plot(x, result.val_loss, label='Validation Loss', linewidth=2)\n",
    "    ax[0].set_title('Loss', fontsize=20)\n",
    "    ax[0].set_xticks(np.arange(0, len(x), 2))\n",
    "    ax[0].legend()\n",
    "\n",
    "    # AX1 : Accuracy\n",
    "    ax[1].plot(x, result.accuracy, label='Accuracy', linewidth=2)\n",
    "    ax[1].plot(x, result.val_accuracy, label='Validation Accuracy', linewidth=2)\n",
    "    ax[1].set_title('Accuracy', fontsize=20)\n",
    "    ax[1].set_xticks(np.arange(0, len(x), 2))\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save accuracy and loss plot\n",
    "    acc_loss_plot_path = os.path.join(output_folder, f\"{prefix}_acc_loss_plot.png\")\n",
    "    plt.savefig(acc_loss_plot_path)\n",
    "    print(f\"Accuracy and Loss plot saved to {acc_loss_plot_path}\")\n",
    "\n",
    "    # Evaluate model\n",
    "    test_loss, test_acc, test_f1score = model.evaluate(test_ds, verbose=1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        predictions = model.predict(images)\n",
    "        y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "        # If labels are one-hot encoded, convert to class indices\n",
    "        if len(labels.shape) > 1:\n",
    "            labels = np.argmax(labels, axis=1)\n",
    "        y_true.extend(labels)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Get the class names from the dataset\n",
    "    class_names = sorted(set(y_true))  # Infer class names from true labels\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    cm_plot_path = os.path.join(output_folder, f\"{prefix}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_plot_path)\n",
    "    print(f\"Confusion Matrix plot saved to {cm_plot_path}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print('Classification Report')\n",
    "    cr = classification_report(y_true, y_pred, digits=5, target_names=[str(c) for c in class_names])\n",
    "    print(cr)\n",
    "\n",
    "    # Save classification report to a text file\n",
    "    cr_path = os.path.join(output_folder, f\"{prefix}_classification_report.txt\")\n",
    "    with open(cr_path, 'w') as f:\n",
    "        f.write('Classification Report\\n')\n",
    "        f.write(cr)\n",
    "    print(f\"Classification Report saved to {cr_path}\")\n",
    "\n",
    "    # Show all plots\n",
    "    plt.show()\n",
    "\n",
    "    return history, test_loss, test_acc, test_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e72b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(result_dict, file_path='results/results.csv'):\n",
    "    # Convert result_dict to DataFrame\n",
    "    df = pd.DataFrame(result_dict)\n",
    "    \n",
    "    # Save the combined DataFrame to CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "# Function to load results from a CSV file\n",
    "def load_results_from_csv(file_path='results/result.csv'):\n",
    "    # If the file exists, load it; otherwise, return an empty dictionary\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        result_dict =  df.to_dict(orient='list')\n",
    "    else:\n",
    "        result_dict =  {\n",
    "            'Model Name': [],\n",
    "            'Test Loss': [],\n",
    "            'Test Accuracy': [],\n",
    "            'Test F1Score': [],\n",
    "            'Training Time': []\n",
    "        }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fac7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(model_name, test_loss, test_acc, test_f1score, result_dict, training_time):\n",
    "    \n",
    "    test_f1score = test_f1score[1] if isinstance(test_f1score, (list, np.ndarray)) else test_f1score\n",
    "    \n",
    "    result_dict['Model Name'].append(model_name)\n",
    "    result_dict['Test Loss'].append(test_loss)\n",
    "    result_dict['Test Accuracy'].append(test_acc)\n",
    "    result_dict['Test F1Score'].append(test_f1score)\n",
    "    result_dict['Training Time'].append(training_time)\n",
    "    \n",
    "    save_results_to_csv(result_dict, 'results/result.csv')\n",
    "\n",
    "result_dict = {\n",
    "    'Model Name': [],\n",
    "    'Test Loss': [],\n",
    "    'Test Accuracy': [],\n",
    "    'Test F1Score': [],\n",
    "    'Training Time': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c5576",
   "metadata": {},
   "source": [
    "# Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_300 = \"train_300/Training\"\n",
    "test_dir_300= \"train_300/Testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee6618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 files belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 15:42:16.402001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17947 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:4e:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNumber of batches in train_full : 179\u001b[0m\n",
      "\u001b[1m\u001b[32m Target : \u001b[0m\n",
      "-----------------------------------\n",
      "\u001b[1m\u001b[34mNumber of  Train  batches : 125\u001b[0m\n",
      "\u001b[1m\u001b[34mNumber of Validation batches : 27\u001b[0m\n",
      "\u001b[1m\u001b[34mNumber of Test batches : 27\u001b[0m\n",
      "Found 1311 files belonging to 4 classes.\n",
      "Found 5712 files belonging to 4 classes.\n",
      "\u001b[1m\u001b[31mNumber of batches in train_full : 357\u001b[0m\n",
      "\u001b[1m\u001b[32m Target : \u001b[0m\n",
      "-----------------------------------\n",
      "\u001b[1m\u001b[34mNumber of  Train  batches : 249\u001b[0m\n",
      "\u001b[1m\u001b[34mNumber of Validation batches : 54\u001b[0m\n",
      "\u001b[1m\u001b[34mNumber of Test batches : 54\u001b[0m\n",
      "Found 1311 files belonging to 4 classes.\n",
      "Found 5712 files belonging to 4 classes.\n",
      "\u001b[1m\u001b[31mNumber of batches in train_full : 2856\u001b[0m\n",
      "\u001b[1m\u001b[32m Target : \u001b[0m\n",
      "-----------------------------------\n",
      "\u001b[1m\u001b[34mNumber of  Train  batches : 1999\u001b[0m\n",
      "\u001b[1m\u001b[34mNumber of Validation batches : 428\u001b[0m\n",
      "\u001b[1m\u001b[34mNumber of Test batches : 428\u001b[0m\n",
      "Found 1311 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds_300, valid_ds_300, test_ds_300 = prepare_dataset(train_dir_300, test_dir_300, 300, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c33b50",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11abb9f0",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4304b4",
   "metadata": {},
   "source": [
    "### EfficientNetV2B0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61d4fe",
   "metadata": {},
   "source": [
    "#### not trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = 'EfficientnetV2B0 Without Augmentation Not Trainable'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_nt = build_model_not_trainable(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_nt, history_std_b0_nt, training_time_1 = train(std_b0_nt, model_name_1, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21317e0",
   "metadata": {},
   "source": [
    "#### trainable first 25% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9537fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2 = \"EfficientnetV2B0 Without Augmentation Trainable First 25% Layer\"\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_tf25 = build_model_trainable_first25(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_tf25, history_std_b0_tf25, training_time_2 = train(std_b0_tf25, model_name_2, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe35f4d",
   "metadata": {},
   "source": [
    "#### trainable first 50% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_3 = 'EfficientnetV2B0 Without Augmentation Trainable First 50% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_tf50 = build_model_trainable_last50(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_tf50, history_std_b0_tf50, training_time_3 = train(std_b0_tf50, model_name_3, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bca14f",
   "metadata": {},
   "source": [
    "#### trainable first 75% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_4 = 'EfficientnetV2B0 Without Augmentation Trainable First 75% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_tf75 = build_model_trainable_last75(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_tf75, history_std_b0_tf75, training_time_4 = train(std_b0_tf75, model_name_4, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c3f05",
   "metadata": {},
   "source": [
    "#### trainable last 25% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d00b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_5 = 'EfficientnetV2B0 Without Augmentation Trainable Last 25% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_tl25 = build_model_trainable_last25(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_tl25, history_std_b0_tl25, training_time_5 = train(std_b0_tl25, model_name_5, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c4bf9",
   "metadata": {},
   "source": [
    "#### trainable last 50% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_6 = 'EfficientnetV2B0 Without Augmentation Trainable Last 50% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_tl50 = build_model_trainable_last50(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_tl50, history_std_b0_tl50, training_time_6 = train(std_b0_tl50, model_name_6, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b5620",
   "metadata": {},
   "source": [
    "#### trainable last 75% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_7 = 'EfficientnetV2B0 Without Augmentation Trainable Last 75% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_tl75 = build_model_trainable_last75(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_tl75, history_std_b0_tl75, training_time_7 = train(std_b0_tl75, model_name_7, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c693c61",
   "metadata": {},
   "source": [
    "#### full trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfc024",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_8 = 'EfficientnetV2B0 Without Augmentation Full Trainable'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b0_ft = build_model_full_trainable(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "std_b0_ft, history_std_b0_ft, training_time_8 = train(std_b0_ft, model_name_8, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (model_name_1, std_b0_nt, history_std_b0_nt, training_time_1),\n",
    "    (model_name_2, std_b0_tf25, history_std_b0_tf25, training_time_2),\n",
    "    (model_name_3, std_b0_tf50, history_std_b0_tf50, training_time_3),\n",
    "    (model_name_4, std_b0_tf75, history_std_b0_tf75, training_time_4),\n",
    "    (model_name_5, std_b0_tl25, history_std_b0_tl25, training_time_5),\n",
    "    (model_name_6, std_b0_tl50, history_std_b0_tl50, training_time_6),\n",
    "    (model_name_7, std_b0_tl75, history_std_b0_tl75, training_time_7),\n",
    "    (model_name_8, std_b0_ft, history_std_b0_ft, training_time_8)\n",
    "]\n",
    "\n",
    "best_model_name, test_loss, test_acc, test_f1score, best_time = best_variant(\n",
    "    models=models,\n",
    "    test_ds=test_ds_300,\n",
    "    output_csv='EfficientNetV2B0 Without Agumentation.csv',\n",
    "    output_folder='results'\n",
    ")\n",
    "\n",
    "config_1 = best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5526faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/result.csv\n",
      "Result Dictionary Table:\n",
      "                                          Model Name  Test Loss  \\\n",
      "0  EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "\n",
      "   Test Accuracy  Test F1Score  Training Time  \n",
      "0       0.985507      0.972536     284.886228  \n"
     ]
    }
   ],
   "source": [
    "result(best_model_name, test_loss, test_acc, test_f1score, result_dict, best_time)\n",
    "\n",
    "# Convert result_dict to a DataFrame\n",
    "df = pd.DataFrame(result_dict)\n",
    "\n",
    "# Convert any TensorFlow tensors in the F1Score column to numpy for better readability\n",
    "df['Test F1Score'] = df['Test F1Score'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(\"Result Dictionary Table:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adae7f",
   "metadata": {},
   "source": [
    "### EfficientNetV2B3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541d287",
   "metadata": {},
   "source": [
    "#### not trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = 'EfficientnetV2B3 Without Augmentation Not Trainable'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_nt = build_model_not_trainable(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_nt, history_std_b3_nt, training_time_1 = train(std_b3_nt, model_name_1, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433601d3",
   "metadata": {},
   "source": [
    "#### trainable first 25% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78562bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2 = 'EfficientnetV2B3 Without Augmentation Trainable First 25% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_tf25 = build_model_trainable_first25(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_tf25, history_std_b3_tf25, training_time_2 = train(std_b3_tf25, model_name_2, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a38c1b",
   "metadata": {},
   "source": [
    "#### trainable first 50% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_3 = 'EfficientnetV2B3 Without Augmentation Trainable First 50% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_tf50 = build_model_trainable_first50(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_tf50, history_std_b3_tf50, training_time_3 = train(std_b3_tf50, model_name_3, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6f8d5",
   "metadata": {},
   "source": [
    "#### trainable first 75% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95832160",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_4 = 'EfficientnetV2B3 Without Augmentation Trainable First 75% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_tf75 = build_model_trainable_first75(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_tf75, history_std_b3_tf75, training_time_4 = train(std_b3_tf75, model_name_4, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb12251",
   "metadata": {},
   "source": [
    "#### trainable last 25% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_5 = 'EfficientnetV2B3 Without Augmentation Trainable Last 25% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_tl25 = build_model_trainable_last25(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_tl25, history_std_b3_tl25, training_time_5 = train(std_b3_tl25, model_name_5, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649cb11",
   "metadata": {},
   "source": [
    "#### trainable last 50% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_6 = 'EfficientnetV2B3 Without Augmentation Trainable Last 50% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_tl50 = build_model_trainable_last50(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_tl50, history_std_b3_tl50, training_time_6 = train(std_b3_tl50, model_name_6, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aceccd9",
   "metadata": {},
   "source": [
    "#### trainable last 75% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a37cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_7 = 'EfficientnetV2B3 Without Augmentation Trainable Last 75% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_tl75 = build_model_trainable_last75(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_tl75, history_std_b3_tl75, training_time_7 = train(std_b3_tl75, model_name_7, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c602322",
   "metadata": {},
   "source": [
    "#### full trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b66a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_8 = 'EfficientnetV2B3 Without Augmentation Trainable Full Trainable'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_b3_ft = build_model_full_trainable(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "std_b3_ft, history_std_b3_ft, training_time_8 = train(std_b3_ft, model_name_8, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ea2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (model_name_1, std_b3_nt, history_std_b3_nt, training_time_1),\n",
    "    (model_name_2, std_b3_tf25, history_std_b3_tf25, training_time_2),\n",
    "    (model_name_3, std_b3_tf50, history_std_b3_tf50, training_time_3),\n",
    "    (model_name_4, std_b3_tf75, history_std_b3_tf75, training_time_4),\n",
    "    (model_name_5, std_b3_tl25, history_std_b3_tl25, training_time_5),\n",
    "    (model_name_6, std_b3_tl50, history_std_b3_tl50, training_time_6),\n",
    "    (model_name_7, std_b3_tl75, history_std_b3_tl75, training_time_7),\n",
    "    (model_name_8, std_b3_ft, history_std_b3_ft, training_time_8)\n",
    "]\n",
    "\n",
    "best_model_name, test_loss, test_acc, test_f1score, best_time = best_variant(\n",
    "    models=models,\n",
    "    test_ds=test_ds_300,\n",
    "    output_csv='EfficientNetV2B3 Without Agumentation.csv',\n",
    "    output_folder='results'\n",
    ")\n",
    "\n",
    "config_2 = best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a66a832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/result.csv\n",
      "Result Dictionary Table:\n",
      "                                          Model Name  Test Loss  \\\n",
      "0  EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "1  EfficientnetV2B3 Without Augmentation Trainabl...   0.053577   \n",
      "\n",
      "   Test Accuracy  Test F1Score  Training Time  \n",
      "0       0.985507      0.972536     284.886228  \n",
      "1       0.992372      0.987055     818.727502  \n"
     ]
    }
   ],
   "source": [
    "result(best_model_name, test_loss, test_acc, test_f1score, result_dict, best_time)\n",
    "\n",
    "# Convert result_dict to a DataFrame\n",
    "df = pd.DataFrame(result_dict)\n",
    "\n",
    "# Convert any TensorFlow tensors in the F1Score column to numpy for better readability\n",
    "df['Test F1Score'] = df['Test F1Score'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(\"Result Dictionary Table:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84214d",
   "metadata": {},
   "source": [
    "### EfficientNetV2M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58e097",
   "metadata": {},
   "source": [
    "#### not trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = 'EfficientnetV2M Withouth Augmentation Not Trainable'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_nt = build_model_not_trainable(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_nt, history_std_m_nt, training_time_1 = train(std_m_nt, model_name_1, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f135f1",
   "metadata": {},
   "source": [
    "#### trainable first 25% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fcd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2 = 'EfficientnetV2M Without Augmentation Trainable First 25% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_tf25 = build_model_trainable_first25(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_tf25, history_std_m_tf25, training_time_2 = train(std_m_tf25, model_name_2, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376d767",
   "metadata": {},
   "source": [
    "#### trainable first 50% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82575b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_3 = 'EfficientnetV2M Without Augmentation Trainable First 50% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_tf50 = build_model_trainable_first50(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_tf50, history_std_m_tf50, training_time_3 = train(std_m_tf50, model_name_3, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25da49",
   "metadata": {},
   "source": [
    "#### trainable first 75% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688aa72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_4 = 'EfficientnetV2M Without Augmentation Trainable First 75% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_tf75 = build_model_trainable_first75(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_tf75, history_std_m_tf75, training_time_4 = train(std_m_tf75, model_name_4, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093df95e",
   "metadata": {},
   "source": [
    "#### trainable last 25% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07337992",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_5 = 'EfficientnetV2M Without Augmentation Trainable Last 25% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_tl25 = build_model_trainable_last25(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_tl25, history_std_m_tl25, training_time_5 = train(std_m_tl25, model_name_5, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6f828",
   "metadata": {},
   "source": [
    "#### trainable last 50% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15311909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_6 = 'EfficientnetV2M Without Augmentation Trainable Last 50% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_tl50 = build_model_trainable_last50(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_tl50, history_std_m_tl50, training_time_6 = train(std_m_tl50, model_name_6, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454cef39",
   "metadata": {},
   "source": [
    "#### trainable last 75% layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a205e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_7 = 'EfficientnetV2M Without Augmentation Trainable Last 75% Layer'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_tl75 = build_model_trainable_last75(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_tl75, history_std_m_tl75, training_time_7 = train(std_m_tl75, model_name_7, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e0719",
   "metadata": {},
   "source": [
    "#### full trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_8 = 'EfficientnetV2M Without Augmentation Full Trainable'\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "std_m_ft = build_model_full_trainable(EfficientNetV2M, input_shape=input_shape, num_classes=4)\n",
    "std_m_ft, history_std_m_ft, training_time_8 = train(std_m_ft, model_name_8, train_ds_300, valid_ds_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf57f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (model_name_1, std_m_nt, history_std_m_nt, training_time_1),\n",
    "    (model_name_2, std_m_tf25, history_std_m_tf25, training_time_2),\n",
    "    (model_name_3, std_m_tf50, history_std_m_tf50, training_time_3),\n",
    "    (model_name_4, std_m_tf75, history_std_m_tf75, training_time_4),\n",
    "    (model_name_5, std_m_tl25, history_std_m_tl25, training_time_5),\n",
    "    (model_name_6, std_m_tl50, history_std_m_tl50, training_time_6),\n",
    "    (model_name_7, std_m_tl75, history_std_m_tl75, training_time_7),\n",
    "    (model_name_8, std_m_ft, history_std_m_ft, training_time_8)\n",
    "]\n",
    "\n",
    "best_model_name, test_loss, test_acc, test_f1score, best_time = best_variant(\n",
    "    models=models,\n",
    "    test_ds=test_ds_300,\n",
    "    output_csv='EfficientNetV2M Without Agumentation.csv',\n",
    "    output_folder='results'\n",
    ")\n",
    "\n",
    "config_3 = best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd31333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/result.csv\n",
      "Result Dictionary Table:\n",
      "                                          Model Name  Test Loss  \\\n",
      "0  EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "1  EfficientnetV2B3 Without Augmentation Trainabl...   0.053577   \n",
      "2  EfficientnetV2M Without Augmentation Trainable...   0.122906   \n",
      "\n",
      "   Test Accuracy  Test F1Score  Training Time  \n",
      "0       0.985507      0.972536     284.886228  \n",
      "1       0.992372      0.987055     818.727502  \n",
      "2       0.982456      0.969305    4399.008303  \n"
     ]
    }
   ],
   "source": [
    "result(best_model_name, test_loss, test_acc, test_f1score, result_dict, best_time)\n",
    "\n",
    "# Convert result_dict to a DataFrame\n",
    "df = pd.DataFrame(result_dict)\n",
    "\n",
    "# Convert any TensorFlow tensors in the F1Score column to numpy for better readability\n",
    "df['Test F1Score'] = df['Test F1Score'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(\"Result Dictionary Table:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280bd16",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1337a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.25),\n",
    "    tf.keras.layers.RandomTranslation(0.15, 0.15),\n",
    "    # tf.keras.layers.RandomZoom(0.2, 0.2),\n",
    "    # tf.keras.layers.RandomBrightness(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c830cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_train(image, label):\n",
    "    image = preprocess_input(image)\n",
    "    image = data_augmentation(image)\n",
    "    return image, label\n",
    "\n",
    "def preprocess_val(image, label):\n",
    "    image = preprocess_input(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_300 = train_ds_300.map(augmentation_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid_aug_300 = valid_ds_300.map(preprocess_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_aug_300 = train_aug_300.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "valid_aug_300 = valid_aug_300.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ad1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_300 = train_aug_300.concatenate(train_ds_300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85626758",
   "metadata": {},
   "source": [
    "### EfficientNetV2B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_1 = config_1.replace(\"Without\", \"with\")\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "aug_b0 = build_model_trainable_first25(EfficientNetV2B0, input_shape=input_shape, num_classes=4)\n",
    "aug_b0, history_aug_b0, training_time_1 = train(aug_b0, model_name_1, train_aug_300, valid_aug_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (model_name_1, model_name_1, history_aug_b0, training_time_1)\n",
    "]\n",
    "\n",
    "best_model_name, test_loss, test_acc, test_f1score, best_time = best_variant(\n",
    "    models=models,\n",
    "    test_ds=test_ds_300,\n",
    "    output_csv='EfficientNetV2B0 With Agumentation.csv',\n",
    "    output_folder='results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e113675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/result.csv\n",
      "Result Dictionary Table:\n",
      "                                          Model Name  Test Loss  \\\n",
      "0  EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "1  EfficientnetV2B3 Without Augmentation Trainabl...   0.053577   \n",
      "2  EfficientnetV2M Without Augmentation Trainable...   0.122906   \n",
      "3  EfficientnetV2B0 With Augmentation Trainable F...   0.133764   \n",
      "\n",
      "   Test Accuracy  Test F1Score  Training Time  \n",
      "0       0.985507      0.972536     284.886228  \n",
      "1       0.992372      0.987055     818.727502  \n",
      "2       0.982456      0.969305    4399.008303  \n",
      "3       0.986270      0.974276     905.791257  \n"
     ]
    }
   ],
   "source": [
    "result(best_model_name, test_loss, test_acc, test_f1score, result_dict, best_time)\n",
    "\n",
    "# Convert result_dict to a DataFrame\n",
    "df = pd.DataFrame(result_dict)\n",
    "\n",
    "# Convert any TensorFlow tensors in the F1Score column to numpy for better readability\n",
    "df['Test F1Score'] = df['Test F1Score'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(\"Result Dictionary Table:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99287a",
   "metadata": {},
   "source": [
    "### EfficientNetV2B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2 = config_2.replace(\"Without\", \"With\")\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "aug_b3 = build_model_trainable_first25(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "aug_b3, history_aug_b3, training_time_2 = train(aug_b3, model_name_2, train_aug_300, valid_aug_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e389cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (model_name_2, model_name_2, history_aug_b3, training_time_2)\n",
    "]\n",
    "\n",
    "best_model_name, test_loss, test_acc, test_f1score, best_time = best_variant(\n",
    "    models=models,\n",
    "    test_ds=test_ds_300,\n",
    "    output_csv='EfficientNetV2B3 With Agumentation.csv',\n",
    "    output_folder='results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c874a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/result.csv\n",
      "Result Dictionary Table:\n",
      "                                           Model Name  Test Loss  \\\n",
      "0   EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "1   EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "2   EfficientnetV2B3 Without Augmentation Trainabl...   0.053577   \n",
      "3   EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "4   EfficientnetV2B3 Without Augmentation Trainabl...   0.053577   \n",
      "5   EfficientnetV2M Without Augmentation Trainable...   0.122906   \n",
      "6   EfficientnetV2B0 Without Augmentation Trainabl...   0.092373   \n",
      "7   EfficientnetV2B3 Without Augmentation Trainabl...   0.053577   \n",
      "8   EfficientnetV2M Without Augmentation Trainable...   0.122906   \n",
      "9   EfficientnetV2B0 With Augmentation Trainable F...   0.133764   \n",
      "10  EfficientnetV2B3 With Augmentation Trainable F...   0.061761   \n",
      "\n",
      "    Test Accuracy  Test F1Score  Training Time  \n",
      "0        0.985507      0.972536     284.886228  \n",
      "1        0.985507      0.972536     284.886228  \n",
      "2        0.992372      0.987055     818.727502  \n",
      "3        0.985507      0.972536     284.886228  \n",
      "4        0.992372      0.987055     818.727502  \n",
      "5        0.982456      0.969305    4399.008303  \n",
      "6        0.985507      0.972536     284.886228  \n",
      "7        0.992372      0.987055     818.727502  \n",
      "8        0.982456      0.969305    4399.008303  \n",
      "9        0.986270      0.974276     905.791257  \n",
      "10       0.993135      0.986971    1513.573711  \n"
     ]
    }
   ],
   "source": [
    "result(best_model_name, test_loss, test_acc, test_f1score, result_dict, best_time)\n",
    "\n",
    "# Convert result_dict to a DataFrame\n",
    "df = pd.DataFrame(result_dict)\n",
    "\n",
    "# Convert any TensorFlow tensors in the F1Score column to numpy for better readability\n",
    "df['Test F1Score'] = df['Test F1Score'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(\"Result Dictionary Table:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada5e91",
   "metadata": {},
   "source": [
    "### EfficientNetV2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_3 = config_3.replace(\"Without\", \"With\")\n",
    "input_shape = (300,300,3)\n",
    "\n",
    "aug_m = build_model_trainable_first25(EfficientNetV2B3, input_shape=input_shape, num_classes=4)\n",
    "aug_m, history_aug_m, training_time_3 = train(aug_m, model_name_3, train_aug_300, valid_aug_300, EPOCHS, SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (model_name_3, model_name_3, history_aug_m, training_time_3)\n",
    "]\n",
    "\n",
    "best_model_name, test_loss, test_acc, test_f1score, best_time = best_variant(\n",
    "    models=models,\n",
    "    test_ds=test_ds_300,\n",
    "    output_csv='EfficientNetV2M With Agumentation.csv',\n",
    "    output_folder='results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523895c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result(best_model_name, test_loss, test_acc, test_f1score, result_dict, best_time)\n",
    "\n",
    "# Convert result_dict to a DataFrame\n",
    "df = pd.DataFrame(result_dict)\n",
    "\n",
    "# Convert any TensorFlow tensors in the F1Score column to numpy for better readability\n",
    "df['Test F1Score'] = df['Test F1Score'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else x)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(\"Result Dictionary Table:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b35ec0",
   "metadata": {},
   "source": [
    "### Combine All Results from Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (model_name_1, model_name_1, history_aug_b0, training_time_1),\n",
    "    (model_name_2, model_name_2, history_aug_b3, training_time_2),\n",
    "    (model_name_3, model_name_3, history_aug_m, training_time_3)\n",
    "]\n",
    "\n",
    "best_model_name, test_loss, test_acc, test_f1score, best_time = best_variant(\n",
    "    models=models,\n",
    "    test_ds=test_ds_300,\n",
    "    output_csv='Agumentation.csv',\n",
    "    output_folder='results'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bf741",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf82033",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(result_dict)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"result.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"result.csv\")\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
